{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 150>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data, columns)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Find highly correlated string columns with a threshold of 0.5\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m correlated_groups \u001b[38;5;241m=\u001b[39m \u001b[43mfind_highly_correlated_string_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHighly Correlated String Columns Groups:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m correlated_groups:\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mfind_highly_correlated_string_columns\u001b[0;34m(df, threshold, missing_placeholder)\u001b[0m\n\u001b[1;32m     83\u001b[0m contingency \u001b[38;5;241m=\u001b[39m df_filled\u001b[38;5;241m.\u001b[39mgroupBy(col1, col2)\u001b[38;5;241m.\u001b[39magg(count(lit(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Calculate chi-squared statistic\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the necessary sums for chi-squared\u001b[39;00m\n\u001b[1;32m     87\u001b[0m contingency_summary \u001b[38;5;241m=\u001b[39m contingency\u001b[38;5;241m.\u001b[39mgroupBy()\u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Total observations\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Unique categories for each column\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     countDistinct(col1)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     92\u001b[0m     countDistinct(col2)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk2\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Sum of squares for observed frequencies\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28msum\u001b[39m(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m*\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m )\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     97\u001b[0m n \u001b[38;5;241m=\u001b[39m contingency_summary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     98\u001b[0m k1 \u001b[38;5;241m=\u001b[39m contingency_summary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/column.py:710\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn is not iterable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import col, count, when, lit, sum as _sum, countDistinct\n",
    "from pyspark.sql.types import StringType\n",
    "from itertools import combinations\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def find_highly_correlated_string_columns(\n",
    "    df: DataFrame, threshold: float = 0.7, missing_placeholder: str = \"__MISSING__\"\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Identifies groups of string (categorical) columns in a PySpark DataFrame that are highly correlated.\n",
    "    Correlation is measured using Cramér's V statistic.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The input PySpark DataFrame containing string columns to analyze.\n",
    "    threshold : float, optional\n",
    "        The Cramér's V threshold above which columns are considered highly correlated.\n",
    "        Defaults to 0.7.\n",
    "    missing_placeholder : str, optional\n",
    "        The string to replace null values with in string columns.\n",
    "        Defaults to \"__MISSING__\".\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    List[List[str]]\n",
    "        A list of groups, where each group is a list of column names that are highly correlated.\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If no string columns are found in the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Identify string columns\n",
    "    string_cols = [\n",
    "        field.name\n",
    "        for field in df.schema.fields\n",
    "        if isinstance(field.dataType, StringType)\n",
    "    ]\n",
    "\n",
    "    if not string_cols:\n",
    "        raise ValueError(\"No string columns found in the DataFrame.\")\n",
    "\n",
    "    # Step 2: Handle missing values by replacing nulls with a placeholder\n",
    "    df_filled = df.select(\n",
    "        [\n",
    "            when(col(c).isNull(), lit(missing_placeholder)).otherwise(col(c)).alias(c)\n",
    "            for c in string_cols\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Cache the filled DataFrame as it will be used multiple times\n",
    "    df_filled.cache()\n",
    "    df_filled_count = df_filled.count()  # Trigger caching\n",
    "\n",
    "    # Precompute the number of unique categories for each column\n",
    "    unique_counts = {}\n",
    "    for c in string_cols:\n",
    "        unique_counts[c] = df_filled.select(c).distinct().count()\n",
    "\n",
    "    # Broadcast the unique counts for efficiency\n",
    "    spark = df_filled.sparkSession\n",
    "    bc_unique_counts = spark.sparkContext.broadcast(unique_counts)\n",
    "\n",
    "    # Initialize Union-Find structure for grouping\n",
    "    parent = {col_name: col_name for col_name in string_cols}\n",
    "\n",
    "    def find(x: str) -> str:\n",
    "        # Path Compression\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "\n",
    "    def union(x: str, y: str) -> None:\n",
    "        root_x = find(x)\n",
    "        root_y = find(y)\n",
    "        if root_x != root_y:\n",
    "            parent[root_y] = root_x\n",
    "\n",
    "    # Step 3: Compute Cramér's V for each pair of string columns\n",
    "    for col1, col2 in combinations(string_cols, 2):\n",
    "        # Create contingency table\n",
    "        contingency = df_filled.groupBy(col1, col2).agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "        # Compute chi-squared statistic\n",
    "        # Step 3.1: Compute the total number of observations\n",
    "        n = df_filled_count\n",
    "\n",
    "        # Step 3.2: Compute the observed frequencies\n",
    "        # To compute chi-squared, we need the observed counts and expected counts\n",
    "        # However, computing expected counts directly is expensive\n",
    "        # Instead, we'll use an approximation for large datasets\n",
    "\n",
    "        # Compute row sums and column sums\n",
    "        row_sums = df_filled.groupBy(col1).agg(_sum(lit(1)).alias(\"row_sum\"))\n",
    "        col_sums = df_filled.groupBy(col2).agg(_sum(lit(1)).alias(\"col_sum\"))\n",
    "\n",
    "        # Join contingency with row_sums and col_sums to compute expected counts\n",
    "        contingency_with_totals = contingency.join(row_sums, on=col1, how=\"left\").join(\n",
    "            col_sums, on=col2, how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Calculate expected counts and chi-squared components\n",
    "        # E_ij = (row_sum_i * col_sum_j) / n\n",
    "        # (O_ij - E_ij)^2 / E_ij\n",
    "        contingency_with_calc = contingency_with_totals.withColumn(\n",
    "            \"expected\", (col(\"row_sum\") * col(\"col_sum\")) / lit(n)\n",
    "        ).withColumn(\n",
    "            \"chi_sq_component\",\n",
    "            ((col(\"count\") - col(\"expected\")) ** 2) / col(\"expected\"),\n",
    "        )\n",
    "\n",
    "        # Sum all chi_sq_components to get chi-squared statistic\n",
    "        chi2 = contingency_with_calc.agg(\n",
    "            _sum(\"chi_sq_component\").alias(\"chi2\")\n",
    "        ).collect()[0][\"chi2\"]\n",
    "\n",
    "        # Calculate Cramér's V\n",
    "        k1 = bc_unique_counts.value[col1]\n",
    "        k2 = bc_unique_counts.value[col2]\n",
    "        min_dim = min(k1, k2) - 1\n",
    "        if min_dim > 0 and n > 0:\n",
    "            cramer_v = math.sqrt(chi2 / (n * min_dim))\n",
    "        else:\n",
    "            cramer_v = 0.0\n",
    "\n",
    "        if cramer_v >= threshold:\n",
    "            union(col1, col2)\n",
    "\n",
    "    # Step 4: Group columns that are highly correlated\n",
    "    group_dict: Dict[str, List[str]] = defaultdict(list)\n",
    "    for col_name in string_cols:\n",
    "        root = find(col_name)\n",
    "        group_dict[root].append(col_name)\n",
    "\n",
    "    # Filter out groups with only one column\n",
    "    groups = [group for group in group_dict.values() if len(group) > 1]\n",
    "\n",
    "    # Unpersist the cached DataFrame\n",
    "    df_filled.unpersist()\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session (if not already initialized)\n",
    "spark = SparkSession.builder.appName(\"CorrelatedStringColumns\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame (for illustration; replace with your actual DataFrame)\n",
    "data = [\n",
    "    (\"A\", \"X\", \"foo\"),\n",
    "    (\"A\", \"Y\", \"bar\"),\n",
    "    (\"B\", \"X\", \"foo\"),\n",
    "    (\"B\", \"Y\", \"bar\"),\n",
    "    (\"A\", \"X\", \"foo\"),\n",
    "    (\"B\", \"Y\", \"bar\"),\n",
    "    (None, \"Z\", None),\n",
    "]\n",
    "columns = [\"col1\", \"col2\", \"col3\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Find highly correlated string columns with a threshold of 0.5\n",
    "correlated_groups = find_highly_correlated_string_columns(df, threshold=0.5)\n",
    "\n",
    "print(\"Highly Correlated String Columns Groups:\")\n",
    "for group in correlated_groups:\n",
    "    print(group)\n",
    "\n",
    "# Stop Spark Session (if needed)\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
